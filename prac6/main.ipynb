{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession  #type: ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"datafram_writer_demo\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration for adding timestamp in cell execution \n",
    "\n",
    "import datetime\n",
    "import functools\n",
    "\n",
    "def log_execution_time(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        execution_time = datetime.datetime.now()\n",
    "        print(f\"Function {func.__name__} executed at: {execution_time}\")\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function my_spark_job executed at: 2025-03-12 13:16:45.464911\n"
     ]
    }
   ],
   "source": [
    "@log_execution_time\n",
    "def my_spark_job():\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Streaming Application\") \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "my_spark_job()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = \"order_id long , order_date string, customer_id long,order_status string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(r\"C:\\Users\\sayed\\Desktop\\DDP\\prac3\\data\\orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|      11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|       7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|       4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|       2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|       5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|       5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|        918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|       1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|       9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|       9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|       2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|       7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|       2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|       1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|       9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|       9198|     PROCESSING|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo1\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo2\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write \\\n",
    ".format(\"json\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\",\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo3\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write \\\n",
    ".format(\"orc\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\",\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo4\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write \\\n",
    ".format(\"json\") \\\n",
    ".mode(\"overwrite\") \\\n",
    ".option(\"path\",\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo4\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write \\\n",
    ".format(\"json\") \\\n",
    ".mode(\"ignore\") \\\n",
    ".option(\"path\",\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo4\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.write \\\n",
    ".format(\"json\") \\\n",
    ".mode(\"append\") \\\n",
    ".option(\"path\",\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo4\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
