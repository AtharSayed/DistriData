{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"datafram_writer_demo\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-6FSOKDO:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>datafram_writer_demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x152288c62d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_schema = \"order_id long , order_date string, cust_id long,order_status string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dataframe = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(r\"C:\\Users\\sayed\\Desktop\\DDP\\prac3\\data\\orders_1gb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+---------------+\n",
      "|order_id|          order_date|cust_id|   order_status|\n",
      "+--------+--------------------+-------+---------------+\n",
      "|       1|2013-07-25 00:00:...|  11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|    256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|  12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|   8827|         CLOSED|\n",
      "|       5|2013-07-25 00:00:...|  11318|       COMPLETE|\n",
      "|       6|2013-07-25 00:00:...|   7130|       COMPLETE|\n",
      "|       7|2013-07-25 00:00:...|   4530|       COMPLETE|\n",
      "|       8|2013-07-25 00:00:...|   2911|     PROCESSING|\n",
      "|       9|2013-07-25 00:00:...|   5657|PENDING_PAYMENT|\n",
      "|      10|2013-07-25 00:00:...|   5648|PENDING_PAYMENT|\n",
      "|      11|2013-07-25 00:00:...|    918| PAYMENT_REVIEW|\n",
      "|      12|2013-07-25 00:00:...|   1837|         CLOSED|\n",
      "|      13|2013-07-25 00:00:...|   9149|PENDING_PAYMENT|\n",
      "|      14|2013-07-25 00:00:...|   9842|     PROCESSING|\n",
      "|      15|2013-07-25 00:00:...|   2568|       COMPLETE|\n",
      "|      16|2013-07-25 00:00:...|   7276|PENDING_PAYMENT|\n",
      "|      17|2013-07-25 00:00:...|   2667|       COMPLETE|\n",
      "|      18|2013-07-25 00:00:...|   1205|         CLOSED|\n",
      "|      19|2013-07-25 00:00:...|   9488|PENDING_PAYMENT|\n",
      "|      20|2013-07-25 00:00:...|   9198|     PROCESSING|\n",
      "+--------+--------------------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dataframe = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\sparkwriterdemo1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dataframe.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2833500|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from orders where order_status = 'CLOSED'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dataframe.write \\\n",
    ".format(\"csv\")\\\n",
    ".mode(\"overwrite\") \\\n",
    ".partitionBy(\"order_status\") \\\n",
    ".option(\"path\", \"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\partitiondemo1\") \\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_dataframe.rdd.getNumPartitions()  # type: ignore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select distinct(order_status) from orders\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".schema(orders_schema) \\\n",
    ".load(\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac3\\\\data\\\\partitiondemo1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_dataframe.createOrReplaceTempView(\"orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2833500|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from orders where order_status = 'CLOSED'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    1875|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from orders where cust_id = 11599\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|     375|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from orders where cust_id = 11599 and order_status = 'CLOSED'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_dataframe = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"inferSchema\", True) \\\n",
    ".load(r\"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\prac2\\\\data\\\\part-00000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_c0', '_c1', '_c2', '_c3']\n"
     ]
    }
   ],
   "source": [
    "print(customers_dataframe.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_final_dataframe = customers_dataframe.toDF(\n",
    "    \"customer_id\", \"customer_fname\", \"customer_lname\", \"customer_email\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# Adding placeholder columns with default values (None or empty strings)\n",
    "customers_dataframe = customers_dataframe \\\n",
    "    .withColumn(\"customer_password\", lit(None)) \\\n",
    "    .withColumn(\"customer_street\", lit(None)) \\\n",
    "    .withColumn(\"customer_city\", lit(None)) \\\n",
    "    .withColumn(\"customer_state\", lit(None)) \\\n",
    "    .withColumn(\"customer_zipcode\", lit(None))\n",
    "\n",
    "customers_final_dataframe = customers_dataframe.toDF(\n",
    "    \"customer_id\", \"customer_fname\", \"customer_lname\", \"customer_email\", \n",
    "    \"customer_password\", \"customer_street\", \"customer_city\", \"customer_state\", \"customer_zipcode\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
