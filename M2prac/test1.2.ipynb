{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cust_id: string (nullable = true)\n",
      " |-- cust_fname: string (nullable = true)\n",
      " |-- cust_lname: string (nullable = true)\n",
      " |-- cust_email: string (nullable = true)\n",
      " |-- cust_password: string (nullable = true)\n",
      " |-- cust_street: string (nullable = true)\n",
      " |-- cust_city: string (nullable = true)\n",
      " |-- cust_state: string (nullable = true)\n",
      " |-- cust_zipcode: string (nullable = true)\n",
      "\n",
      "+-------+----------+----------+----------+-------------+--------------------+-----------+----------+------------+\n",
      "|cust_id|cust_fname|cust_lname|cust_email|cust_password|         cust_street|  cust_city|cust_state|cust_zipcode|\n",
      "+-------+----------+----------+----------+-------------+--------------------+-----------+----------+------------+\n",
      "|      1|   Richard| Hernandez| XXXXXXXXX|    XXXXXXXXX|  6303 Heather Plaza|Brownsville|        TX|       78521|\n",
      "|      2|      Mary|   Barrett| XXXXXXXXX|    XXXXXXXXX|9526 Noble Embers...|  Littleton|        CO|       80126|\n",
      "|      3|       Ann|     Smith| XXXXXXXXX|    XXXXXXXXX|3422 Blue Pioneer...|     Caguas|        PR|       00725|\n",
      "|      4|      Mary|     Jones| XXXXXXXXX|    XXXXXXXXX|  8324 Little Common| San Marcos|        CA|       92069|\n",
      "|      5|    Robert|    Hudson| XXXXXXXXX|    XXXXXXXXX|10 Crystal River ...|     Caguas|        PR|       00725|\n",
      "+-------+----------+----------+----------+-------------+--------------------+-----------+----------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-----+\n",
      "|cust_lname|count|\n",
      "+----------+-----+\n",
      "|     Smith| 4626|\n",
      "|   Johnson|   76|\n",
      "|  Williams|   69|\n",
      "|     Jones|   65|\n",
      "|     Brown|   62|\n",
      "+----------+-----+\n",
      "\n",
      "+-------+----------+----------+----------+-------------+-----------+---------+----------+------------+\n",
      "|cust_id|cust_fname|cust_lname|cust_email|cust_password|cust_street|cust_city|cust_state|cust_zipcode|\n",
      "+-------+----------+----------+----------+-------------+-----------+---------+----------+------------+\n",
      "+-------+----------+----------+----------+-------------+-----------+---------+----------+------------+\n",
      "\n",
      "+-------------+--------------+\n",
      "|    cust_city|customer_count|\n",
      "+-------------+--------------+\n",
      "|  Los Angeles|           224|\n",
      "|    San Diego|           104|\n",
      "|     San Jose|            71|\n",
      "|  Bakersfield|            41|\n",
      "|    Santa Ana|            36|\n",
      "|   Long Beach|            36|\n",
      "|    Escondido|            29|\n",
      "|       Fresno|            29|\n",
      "|      Ontario|            29|\n",
      "|San Francisco|            28|\n",
      "|    Riverside|            27|\n",
      "|Mission Viejo|            26|\n",
      "|    Oceanside|            24|\n",
      "|   Sacramento|            23|\n",
      "|      Modesto|            23|\n",
      "|      Fremont|            22|\n",
      "|Moreno Valley|            21|\n",
      "|      Hayward|            21|\n",
      "|  Simi Valley|            20|\n",
      "| Garden Grove|            20|\n",
      "+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Q2. Customer Dataset \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, length, desc\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Initializing  Spark session with Hive support for using SQL in a better way \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CustomerDataAnalysis\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Since the headers are not present we will manually add the headers in the code which is provided in the question \n",
    "# (cust_id,cust_fname,cust_lname,cust_email,cust_password,cust_street,cust_city,cust_state,cust_zipcode)\n",
    "# Define the correct schema manually.\n",
    "#  \n",
    "# Manually defining the column names for the csv \n",
    "schema = StructType([\n",
    "    StructField(\"cust_id\", StringType(), True),\n",
    "    StructField(\"cust_fname\", StringType(), True),\n",
    "    StructField(\"cust_lname\", StringType(), True),\n",
    "    StructField(\"cust_email\", StringType(), True),\n",
    "    StructField(\"cust_password\", StringType(), True),\n",
    "    StructField(\"cust_street\", StringType(), True),\n",
    "    StructField(\"cust_city\", StringType(), True),\n",
    "    StructField(\"cust_state\", StringType(), True),\n",
    "    StructField(\"cust_zipcode\", StringType(), True)\n",
    "])\n",
    "\n",
    "file_path = r\"C:\\Users\\sayed\\Desktop\\DDP\\M2prac\\data\\part-00000 (1)\"\n",
    "df = spark.read.option(\"header\", \"false\").schema(schema).csv(file_path)\n",
    "\n",
    "# Display the corrected schema\n",
    "df.printSchema()\n",
    "\n",
    "# Check if the first row is incorrect (i.e., a misplaced header)\n",
    "df.show(5)  # Inspect the first few rows\n",
    "\n",
    "# If the first row is incorrect (not actual data), remove it\n",
    "first_row = df.limit(1)\n",
    "df = df.subtract(first_row)\n",
    "\n",
    "# Create a temporary view for Spark SQL\n",
    "df.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "# 1. Find the top 5 most common last names\n",
    "top_last_names = spark.sql(\"\"\"\n",
    "    SELECT cust_lname, COUNT(cust_id) AS count\n",
    "    FROM customers\n",
    "    GROUP BY cust_lname\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "top_last_names.show()\n",
    "\n",
    "# 2. Find invalid zip codes (not 5 digits)\n",
    "invalid_zipcodes = spark.sql(\"\"\"\n",
    "    SELECT * \n",
    "    FROM customers\n",
    "    WHERE LENGTH(cust_zipcode) != 5 OR cust_zipcode RLIKE '[^0-9]'\n",
    "\"\"\")\n",
    "invalid_zipcodes.show()\n",
    "\n",
    "# 3. Count customers per city in California (CA)\n",
    "cust_count_CA = spark.sql(\"\"\"\n",
    "    SELECT cust_city, COUNT(cust_id) AS customer_count\n",
    "    FROM customers\n",
    "    WHERE cust_state = 'CA'\n",
    "    GROUP BY cust_city\n",
    "    ORDER BY customer_count DESC\n",
    "\"\"\")\n",
    "cust_count_CA.show()\n",
    "\n",
    "# Save results to Hive for further use\n",
    "top_last_names.write.mode(\"overwrite\").saveAsTable(\"top_last_names\")\n",
    "invalid_zipcodes.write.mode(\"overwrite\").saveAsTable(\"invalid_zipcodes\")\n",
    "cust_count_CA.write.mode(\"overwrite\").saveAsTable(\"cust_count_CA\")\n",
    "\n",
    "# Stopping  Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
