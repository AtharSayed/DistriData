{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M-2 Test DDP \n",
    "# Name: Athar Sayed \n",
    "# Date : 19/3/2025\n",
    "from pyspark.sql import SparkSession # Starting the Spark Session \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"M-2 Test_DDP\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in permissive mode\n"
     ]
    }
   ],
   "source": [
    "# Giving the JSON file path from the data folder\n",
    "file_path = \"C:\\\\Users\\\\sayed\\\\Desktop\\\\DDP\\\\M2prac\\\\data\\\\sales_data.json\"\n",
    "\n",
    "# There are 3 ways to read the file they are as follows : \n",
    "# We use PERMISSIVE if we want to analyze and clean corrupt records.\n",
    "# We use DROPMALFORMED when only clean data should be processed.\n",
    "# We use FAILFAST in production to prevent bad data from being used.\n",
    "\n",
    "# 1) Since the data might be corrupted or malformed, we will read it in permissive mode\n",
    "print(\"Reading in permissive mode\")\n",
    "data = spark.read.option(\"mode\", \"PERMISSIVE\").json(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in dropmalformed mode: \n",
      "+----------+--------+-------+--------+\n",
      "|product   |quantity|revenue|store_id|\n",
      "+----------+--------+-------+--------+\n",
      "|Apple     |10      |100.0  |1       |\n",
      "|Banana    |15      |75.0   |2       |\n",
      "|Orange    |12      |90.0   |3       |\n",
      "|Mango     |8       |120.0  |4       |\n",
      "|Grape     |20      |150.0  |5       |\n",
      "|Watermelon|5       |50.0   |6       |\n",
      "|Strawberry|18      |108.0  |7       |\n",
      "|Pineapple |14      |140.0  |8       |\n",
      "|Cherry    |7       |105.0  |9       |\n",
      "|Pear      |9       |81.0   |10      |\n",
      "|Blueberry |11      |88.0   |11      |\n",
      "|Kiwi      |16      |128.0  |12      |\n",
      "|Peach     |13      |91.0   |13      |\n",
      "|Plum      |6       |54.0   |14      |\n",
      "|Lemon     |10      |70.0   |15      |\n",
      "|Raspberry |17      |136.0  |16      |\n",
      "|Coconut   |4       |80.0   |17      |\n",
      "|Avocado   |11      |99.0   |18      |\n",
      "|Blackberry|8       |64.0   |19      |\n",
      "|G         |Invalid |NaN    |20      |\n",
      "+----------+--------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2) Reading the file in DROPMALFORMED MODE \n",
    "print(\"Reading in dropmalformed mode: \")\n",
    "\n",
    "data = spark.read.option(\"mode\", \"DROPMALFORMED\").json(file_path)\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading with Failfast mode : \n",
      "FAILFAST Mode Error: An error occurred while calling o254.json.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 44) (DESKTOP-6FSOKDO executor driver): org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInSchemaInferenceError(QueryExecutionErrors.scala:1521)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.handleJsonErrorsByParseMode(JsonInferSchema.scala:68)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:97)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)\n",
      "\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)\n",
      "\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 1])\n",
      " at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 74]\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:697)\n",
      "\tat com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:512)\n",
      "\tat com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:529)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:3103)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:757)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.inferField(JsonInferSchema.scala:185)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$4(JsonInferSchema.scala:92)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:48)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:90)\n",
      "\t... 31 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.infer(JsonInferSchema.scala:120)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.$anonfun$inferFromDataset$5(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.inferFromDataset(JsonDataSource.scala:109)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$.infer(JsonDataSource.scala:98)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonDataSource.inferSchema(JsonDataSource.scala:64)\n",
      "\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat.inferSchema(JsonFileFormat.scala:59)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:362)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor58.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "Caused by: org.apache.spark.SparkException: Malformed records are detected in schema inference. Parse Mode: FAILFAST.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInSchemaInferenceError(QueryExecutionErrors.scala:1521)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.handleJsonErrorsByParseMode(JsonInferSchema.scala:68)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:97)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeft(TraversableOnce.scala:237)\n",
      "\tat scala.collection.TraversableOnce.reduceLeft$(TraversableOnce.scala:220)\n",
      "\tat scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption(TraversableOnce.scala:249)\n",
      "\tat scala.collection.TraversableOnce.reduceLeftOption$(TraversableOnce.scala:248)\n",
      "\tat scala.collection.AbstractIterator.reduceLeftOption(Iterator.scala:1431)\n",
      "\tat scala.collection.TraversableOnce.reduceOption(TraversableOnce.scala:256)\n",
      "\tat scala.collection.TraversableOnce.reduceOption$(TraversableOnce.scala:256)\n",
      "\tat scala.collection.AbstractIterator.reduceOption(Iterator.scala:1431)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$1(JsonInferSchema.scala:107)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\t... 1 more\n",
      "Caused by: com.fasterxml.jackson.core.io.JsonEOFException: Unexpected end-of-input: expected close marker for Object (start marker at [Source: (byte[])\"{\"store_id\": 21, \"product\": \"Pineapple\", \"quantity\": 14, \"revenue\": 140.0\"; line: 1, column: 1])\n",
      " at [Source: UNKNOWN; line: 1, column: 74]\n",
      "\tat com.fasterxml.jackson.core.base.ParserMinimalBase._reportInvalidEOF(ParserMinimalBase.java:697)\n",
      "\tat com.fasterxml.jackson.core.base.ParserBase._handleEOF(ParserBase.java:512)\n",
      "\tat com.fasterxml.jackson.core.base.ParserBase._eofAsNextChar(ParserBase.java:529)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser._skipWSOrEnd(UTF8StreamJsonParser.java:3103)\n",
      "\tat com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:757)\n",
      "\tat org.apache.spark.sql.catalyst.json.JacksonUtils$.nextUntil(JacksonUtils.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.inferField(JsonInferSchema.scala:185)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$4(JsonInferSchema.scala:92)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:48)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)\n",
      "\tat org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.json.JsonInferSchema.$anonfun$infer$2(JsonInferSchema.scala:90)\n",
      "\t... 31 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Reading the data in FAILFAST mode :\n",
    "print(\"Reading with Failfast mode : \")\n",
    "\n",
    "# Using Try and except for exception handling \n",
    "try:\n",
    "    data = spark.read.option(\"mode\", \"FAILFAST\").json(file_path)\n",
    "    data.show(truncate=False)\n",
    "except Exception as e:\n",
    "    print(f\"FAILFAST Mode Error: {e}\")\n",
    "\n",
    "# Here the failfast mode threw an error because while parsing the data there was a mistake in the json file not being corrected closed with this } \n",
    "# in the record \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
